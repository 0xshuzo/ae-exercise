# Description

## Data structure
Before calling the sorter, a data structure is created where the data, to be sorted, is split into buckets. This is done by choosing a value `B` (how many highest bits to consider to split elements) such that every bucket has around 100,000 elements, roughly the size of the L2 or L3 cache of a core. After determining `B`, we mask out the highest `B` bits of every element and use this to assign each element to a bucket. Since the data can be considered as being uniformly distributed, every bucket should contain nearly the same number of elements after this step because the probability of a bit being 0 or 1 is equal (0.5 for each). The final data structure is then passed to the sorter.

## Sorter
The sorter first checks how many threads should be used. If only a single is available, it just sorts all buckets sequentially and concatenates them, otherwise, it spawns `num_threads` threads. The threads are then scheduled to sort a bucket and after finishing, it picks the next unsorted bucket. In the end, all buckets are concatenated. The linear concatenation is possible since we split the elements with respect to the highest `B` bits, therefore we know that all elements in bucket `i` are smaller than all elements in bucket `i+1`.  
The sorting itself is done by looping over all bits, starting with the highest bit set (determined by `start_bit_for_block`). For every bit, we search the first element (starting at index `left` going to the right) with a bit at position `bit` set to 1 and the first element (starting at index `right` going to the left) with a bit at position `bit` set to 0. Those elements are then swapped. This process is repeated until left and right meet. At that point, we recursively sort the 0-part and the 1-part with their respective ranges until we only have at most 16 elements left per range. With at most 16 elements left, we sort the remaining elements using Robin Hood Sort. This sorting algorithm creates a buffer of size `2.5 * elements` and just puts all elements at their respective index with respect to the distance between the smallest and highest element. Collisions are solved by moving higher elements up inside the buffer. In the end, the elements are written back with respect to the order inside the buffer.

## Parameters and respective effects
For choosing good initial parameters, ChatGPT was asked for a good starting point. After that, the parameters were varied a bit to see how the running time per element changes. For example, we break the MSD Radix Sort at 16 elements since this value gave the best results in the experiments. When looking at the `plot.pdf`, one can see that for a single thread the time per element increases roughly logarithmically. When using multiple threads the running time per element starts high and converges around $10^6$ elements and then remains nearly constant. One can also see that the best overall performance is achieved with 8 cores.

## Further possible optimizations
Currently, the creation of the container data structure is done sequentially. This could be also done in parallel by splitting the initial data into `num_threads` blocks so that each thread counts the elements per bucket in its block. The elements are then written into their respective bucket.